{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b2f8e7",
   "metadata": {},
   "source": [
    "First of all, user should catch traffic in his network with \"WireShark\". I think that about 2GB of traffic will be great.\n",
    "And we should separate our pcap file into sessions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import ipaddress\n",
    "\n",
    "from scapy.all import (\n",
    "    PacketList,\n",
    "    Packet,\n",
    "    PcapNgReader,\n",
    "    PcapNgWriter,\n",
    "    Raw,\n",
    ")\n",
    "from typing import List, DefaultDict\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71381280",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 32\n",
    "d_ff = d_model * 4  # Generally set to a value about four times that of d_model\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "dropout = 0.1\n",
    "max_seq_length = 128\n",
    "params = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29bdafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_printoptions(precision=6, sci_mode=False)\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from performer_pytorch import SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fd0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_packet(*, packet: Packet) -> bool:\n",
    "    if not packet.haslayer(\"IP\"):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43082a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sessions(*, packages: PacketList, output_directory: str) -> int:\n",
    "    sessions: DefaultDict[str, List[Packet]] = defaultdict(list)\n",
    "\n",
    "    print(\"\\n==== Distribution of packages by session ====\")\n",
    "\n",
    "    for packet in tqdm(packages):\n",
    "        if not is_valid_packet(packet=packet):\n",
    "            continue\n",
    "\n",
    "        ip_src: str = packet[\"IP\"].src\n",
    "        ip_dst: str = packet[\"IP\"].dst\n",
    "\n",
    "        if Raw in packet:\n",
    "            del packet[Raw]\n",
    "\n",
    "        f_key = f\"{ip_src}-{ip_dst}\"\n",
    "        if f_key in sessions:\n",
    "            sessions[f_key].append(packet)\n",
    "        else:\n",
    "            sessions[f\"{ip_dst}-{ip_src}\"].append(packet)\n",
    "\n",
    "    i = 1\n",
    "    while os.path.exists(output_directory):\n",
    "        output_directory += f\"_{i}\"\n",
    "        i += 1\n",
    "    os.mkdir(output_directory)\n",
    "\n",
    "    print(\"\\n==== Writing packages to files ====\")\n",
    "    for session_key, sessions_packets in tqdm(\n",
    "        sorted(sessions.items(), key=lambda x: len(x[1]))[-max_seq_length:]\n",
    "    ):\n",
    "        filename = f\"{output_directory}/{session_key}\"\n",
    "\n",
    "        i = 1\n",
    "        while os.path.exists(filename):\n",
    "            filename += f\"_{i}\"\n",
    "            i += 1\n",
    "\n",
    "        write_pcap = PcapNgWriter(filename=f\"{filename}.pcapng\")\n",
    "        write_pcap.write(sessions_packets[-max_seq_length:])\n",
    "        write_pcap.flush()\n",
    "        write_pcap.close()\n",
    "\n",
    "    print(\"==== Finished writing packages to files ====\")\n",
    "\n",
    "    return len(sessions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./pcaps/network-traffic-05-21-2025.pcapng\"\n",
    "pcap_file: PcapNgReader = PcapNgReader(file_path)\n",
    "packages: PacketList = pcap_file.read_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51102f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sessions(packages=packages, output_directory=\"./pcaps/sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ea8bb",
   "metadata": {},
   "source": [
    "## Нужно прочитать все сессии и превратить их в датасет\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_packet(\n",
    "    *,\n",
    "    ip_src: float,\n",
    "    ip_dst: float,\n",
    "    mac_src: float,\n",
    "    mac_dst: float,\n",
    "    ip_len: float,\n",
    "    ip_proto: float,\n",
    "    sport: float,\n",
    "    dport: float\n",
    ") -> List[float]:\n",
    "    return [ip_src, ip_dst, mac_src, mac_dst, ip_len, ip_proto, sport, dport][:params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_session(*, session_path: str) -> Tensor:\n",
    "    packets: List[List[float]] = []\n",
    "\n",
    "    max_count = 65535\n",
    "    k = 2**32\n",
    "    t = 2**48\n",
    "\n",
    "    for packet in PcapNgReader(filename=session_path):\n",
    "        if len(packets) == max_seq_length:\n",
    "            break\n",
    "\n",
    "        if not is_valid_packet(packet=packet):\n",
    "            continue\n",
    "\n",
    "        ip_src: float = int(ipaddress.IPv4Address(packet[\"IP\"].src)) / k\n",
    "        ip_dst: float = int(ipaddress.IPv4Address(packet[\"IP\"].dst)) / k\n",
    "\n",
    "        mac_src: float = int(packet.src.replace(\":\", \"\"), 16) / t\n",
    "        mac_dst: float = int(packet.dst.replace(\":\", \"\"), 16) / t\n",
    "\n",
    "        ip_len: float = packet[\"IP\"].len / max_count\n",
    "        ip_proto: float = packet[\"IP\"].proto / 255.0\n",
    "        sport: float = packet.sport / max_count if hasattr(packet, \"sport\") else 0\n",
    "        dport: float = packet.dport / max_count if hasattr(packet, \"dport\") else 0\n",
    "\n",
    "        X = build_packet(\n",
    "            ip_src=ip_src,\n",
    "            ip_dst=ip_dst,\n",
    "            mac_src=mac_src,\n",
    "            mac_dst=mac_dst,\n",
    "            ip_len=ip_len,\n",
    "            ip_proto=ip_proto,\n",
    "            sport=sport,\n",
    "            dport=dport,\n",
    "        )\n",
    "\n",
    "        packets.append(X)\n",
    "\n",
    "    return torch.tensor(packets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcdec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_path = \"./pcaps/sessions\"\n",
    "session_paths: List[str] = [\n",
    "    os.path.join(sessions_path, file) for file in os.listdir(sessions_path)\n",
    "]\n",
    "\n",
    "tensor_sessions: List[Tensor] = []\n",
    "\n",
    "for path in session_paths:\n",
    "    tensor_sessions.append(read_session(session_path=path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c662f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(*, lengths: List[int], max_len: int):\n",
    "    batch_size = len(lengths)\n",
    "    # You have to mask a whole packet instead of each param in this packet.\n",
    "    # So shape of mask is (batch, max_len) and dataset shape is (batch, max_len, d_model)\n",
    "    mask = torch.zeros((batch_size, max_len), dtype=torch.bool)\n",
    "    for i, l in enumerate(lengths):\n",
    "        mask[i, l:] = True\n",
    "    return mask  # shape=(batch, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tensor(*, X: Tensor, filename: str) -> None:\n",
    "    torch.save(X, f\"{filename}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequence(tensor_sessions, batch_first=True)\n",
    "lengths = [session.size(0) for session in tensor_sessions]\n",
    "# max_len = X.size(1)  # shape=(sessions, packets, params)\n",
    "X_mask = generate_mask(lengths=lengths, max_len=max_seq_length)\n",
    "\n",
    "os.mkdir(\"./pcaps/dataset\")\n",
    "save_tensor(X=X, filename=\"./pcaps/dataset/X\")\n",
    "save_tensor(X=X_mask, filename=\"./pcaps/dataset/X_mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28140542",
   "metadata": {},
   "source": [
    "## Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3779e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d56dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PacketEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode packet with params into one vector with dimensional equal to dimensionality of the model\n",
    "\n",
    "    :param nn: Module from pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, params: int):\n",
    "        super(PacketEmbedding, self).__init__()\n",
    "        # Use linear layer to convert packet parameters and normalization layer\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(params, self.d_model), nn.ReLU(), nn.LayerNorm(self.d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        \"\"\"Transform packet into vector with dimensional equal to model\n",
    "\n",
    "        :param x: Tensor with packet - shape=[params]\n",
    "        :return: New vector with shape [d_model]\n",
    "        \"\"\"\n",
    "        return self.embed(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        if d_model <= 0 or num_heads <= 0:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim and num_heads must be greater than 0,\"\n",
    "                f\" got embed_dim={d_model} and num_heads={num_heads} instead\"\n",
    "            )\n",
    "\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # d_model = num_heads * head_dim\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # Query matrix\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # Key matrix\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # Value matrix\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Output matrix\n",
    "\n",
    "    def split_heads(self, X: Tensor) -> Tensor:\n",
    "        # (batch, max_seq_len, d_model)\n",
    "        batch, seq_length, _ = X.size()\n",
    "        return X.view(batch, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, X: Tensor) -> Tensor:\n",
    "        # (batch, num_heads, seq_len, head_dim)\n",
    "        batch, _, seq_len, _ = X.size()\n",
    "        return X.transpose(1, 2).contiguous().view(batch, seq_len, self.d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(\n",
    "        self, Q: Tensor, K: Tensor, V: Tensor, mask: Tensor\n",
    "    ):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # attn_scores.shape = (batch, heads, seq_len, seq_len)\n",
    "\n",
    "        # query_mask = mask[:, None, :, None]  # shape: (batch, 1, seq_len, 1)\n",
    "        # key_mask = mask[:, None, None, :]  # shape: (batch, 1, 1, seq_len)\n",
    "        # new_mask = query_mask | key_mask  # логическое ИЛИ по двум маскам\n",
    "\n",
    "        new_mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "        attn_scores = attn_scores.masked_fill(new_mask == True, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        assert not torch.isnan(attn_probs).any(), \"NaN in attention probabilities!\"\n",
    "        assert not torch.isinf(attn_probs).any(), \"Inf in attention probabilities!\"\n",
    "\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def forward(self, Q: Tensor, K: Tensor, V: Tensor, mask: Tensor):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attention = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attention))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int) -> None:\n",
    "        super(PositionalWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.layer_one = nn.Linear(d_model, d_ff)\n",
    "        self.layer_two = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        return self.layer_two(self.relu(self.layer_one(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b0c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq_length: int):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * -(math.log(100.0) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        assert isinstance(self.pe, Tensor)  # Assert to avoid a Pylance type error\n",
    "        return X + self.pe[:, : X.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, num_heads: int, dropout: float) -> None:\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        # self.attention = SelfAttention(\n",
    "        #     dim=d_model,\n",
    "        #     heads=num_heads,\n",
    "        #     causal=False,\n",
    "        #     nb_features=64,\n",
    "        #     generalized_attention=True,\n",
    "        # )\n",
    "        self.feed_forward = PositionalWiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        self.norm_one = nn.LayerNorm(d_model)\n",
    "        self.norm_two = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X: Tensor, mask: Tensor):\n",
    "        attention = self.attention(X, X, X, mask=mask)\n",
    "        # attention = self.attention(X, mask=mask)\n",
    "        X = self.norm_one(X + self.dropout(attention))\n",
    "\n",
    "        ff_output = self.feed_forward(X)\n",
    "        X = self.norm_two(X + self.dropout(ff_output))\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        d_ff: int,\n",
    "        max_seq_length: int,\n",
    "        params: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder_embedding = PacketEmbedding(d_model, params)\n",
    "        # self.encoder_embedding = nn.Embedding(1000, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, d_ff, num_heads, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # self.decoder = nn.Sequential(\n",
    "        #     nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, params)\n",
    "        # )\n",
    "        self.decoder = nn.Linear(d_model, params)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def define_device(\n",
    "        self,\n",
    "    ) -> str:\n",
    "        curr_accelerator = torch.accelerator.current_accelerator()\n",
    "        device = (\n",
    "            curr_accelerator.type\n",
    "            if curr_accelerator and torch.accelerator.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        return device\n",
    "\n",
    "    def forward(self, X: Tensor, X_mask: Tensor) -> Tensor:\n",
    "        # device = self.define_device()\n",
    "        # X.to(device)\n",
    "        # X_mask.to(device)\n",
    "\n",
    "        # print(f\"{X_embedded.std().item()=}\\n{X_embedded.mean().item()=}\")\n",
    "        X_encoded: Tensor = self.dropout(self.positional_encoding(self.encoder_embedding(X)))\n",
    "\n",
    "        X_out = X_encoded\n",
    "        for layer in self.encoder_layers:\n",
    "            X_out = layer(X_out, X_mask)\n",
    "\n",
    "        return self.decoder(X_out)  # (sessions, d_model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(*, filename: str) -> Tensor:\n",
    "    return torch.load(f\"{filename}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_tensor(filename=\"./pcaps/dataset/X\")\n",
    "X_mask = load_tensor(filename=\"./pcaps/dataset/X_mask\")\n",
    "\n",
    "print(X.shape)\n",
    "print(X_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "def get_warmup_scheduler(\n",
    "    optimizer: Optimizer, d_model: int, warmup_steps=1000\n",
    ") -> LambdaLR:\n",
    "    def lr_lambda(step: int) -> float:\n",
    "        step = max(step, 1)\n",
    "        return (d_model**-0.5) * min(step**-0.5, step * warmup_steps**-1.5)\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    d_model, num_heads, num_layers, d_ff, max_seq_length, params, dropout\n",
    ")\n",
    "transformer.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e4c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=0.001\n",
    ")\n",
    "scheduler = get_warmup_scheduler(optimizer, d_model)\n",
    "\n",
    "transformer.train()\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    X_out: Tensor = transformer(X, X_mask)  # (sessions, max_seq_length, 8)\n",
    "\n",
    "    mask = X_mask.unsqueeze(-1)\n",
    "    mse: Tensor = nn.functional.mse_loss(X_out, X, reduction=\"none\").mean()\n",
    "    masked_mse = mse * mask\n",
    "    loss = masked_mse.sum() / mask.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    print(\"mask sum:\", mask.sum().item())\n",
    "    print(f\"{masked_mse.sum().item()=}\")\n",
    "    print(\"loss:\", loss.item())\n",
    "    print(\n",
    "        \"grad norm:\",\n",
    "        sum(\n",
    "            p.grad.norm().item() for p in transformer.parameters() if p.grad is not None\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # for name, param in transformer.named_parameters():\n",
    "    #     print(name, param.requires_grad)\n",
    "\n",
    "    # for name, param in transformer.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(name, param.grad.abs().mean().item())  # средняя величина градиента\n",
    "    #     else:\n",
    "    #         print(name, \"NO GRAD\")\n",
    "\n",
    "    # print(f'X std: {X.std().item():.4f}, X_out std: {X_out.std().item():.4f}')\n",
    "    # print(f'X mean: {X.mean().item():.4f}, X_out mean: {X_out.mean().item():.4f}')\n",
    "    # print(f'Gradients: {[p.grad.abs().mean().item() if p.grad is not None else 0.0 for p in transformer.parameters() if p.requires_grad]}\\n')\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch: {epoch+1}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
